from langchain_google_genai import ChatGoogleGenerativeAI
import pandas as pd
import json
import streamlit as st

from utils import get_data_profile
from agents.guardian import run_guardian_query
from agents.sage import get_sage_interpretation
from agents.artisan import create_static_plot

def handle_general_conversation(llm: ChatGoogleGenerativeAI, user_query: str) -> str:
    """
    Lida com conversas gerais, sauda√ß√µes e perguntas n√£o relacionadas a dados.
    """
    prompt = f"""Voc√™ √© JEDI, um assistente de an√°lise de dados tem√°tico de Star Wars.
Responda √† pergunta do usu√°rio de forma breve, amig√°vel e dentro do tema.

Pergunta do usu√°rio: "{user_query}"
Sua resposta:
"""
    try:
        response = llm.invoke(prompt)
        return response.content.strip()
    except Exception as e:
        return f"A For√ßa est√° perturbada. N√£o consegui processar a conversa. Erro: {str(e)}"

def run_jedi_council(llm: ChatGoogleGenerativeAI, df: pd.DataFrame, user_query: str, record_thoughts: bool = False):
    """
    Executa a orquestra√ß√£o do Conselho Jedi com esclarecimento com estado.

    Fluxo de L√≥gica:
    1. Verifica se h√° uma pergunta de esclarecimento pendente e a trata.
    2. Se n√£o, usa um LLM para classificar a inten√ß√£o do usu√°rio e escolher uma ferramenta (Guardian, Visualizer, GeneralConversation).
    3. Para tarefas de dados/visualiza√ß√£o, usa um LLM para atuar como um 'Consultor', avaliando a ambiguidade da pergunta ou sugerindo uma abordagem melhor.
    4. Se a pergunta for amb√≠gua, retorna uma pergunta de esclarecimento e salva o estado.
    5. Se a pergunta for clara, executa a ferramenta apropriada.
    6. Envia o resultado da ferramenta para o S√°bio para uma interpreta√ß√£o final em linguagem natural.
    7. Retorna um dicion√°rio contendo a resposta final, o caminho para qualquer artefato e o log de pensamentos (Di√°rio de Bordo).
    """
    log_entries = []
    def log(message):
        if record_thoughts:
            log_entries.append(message)

    if "pending_clarification" in st.session_state and st.session_state.pending_clarification:
        pending_data = st.session_state.pending_clarification
        original_query = pending_data["original_query"]
        intended_tool = pending_data["intended_tool"]
        
        log("ü§î **Pensamento:** O usu√°rio respondeu a uma pergunta de esclarecimento. Combinando o contexto.")
        clarified_query = f"A pergunta original era '{original_query}'. O usu√°rio agora esclareceu com: '{user_query}'. Execute a tarefa original com este novo esclarecimento."
        
        st.session_state.pending_clarification = None
        
        log(f"üé¨ **A√ß√£o:** Acionando a ferramenta '{intended_tool}' com a consulta esclarecida.")
        tool_response = {}
        if intended_tool == "DataGuardian":
            tool_response = run_guardian_query(llm, df, clarified_query, record_thoughts)
        elif intended_tool == "Visualizer":
            tool_response = create_static_plot(llm, df, clarified_query, record_thoughts)
        
        tool_result = tool_response.get("result", "")
        specialist_thoughts = tool_response.get("thoughts", "")
        if record_thoughts and specialist_thoughts:
            log(f"--- In√≠cio do Log Detalhado de {intended_tool} ---")
            log(f"```\n{specialist_thoughts}\n```")
            log(f"--- Fim do Log Detalhado de {intended_tool} ---")

        log("ü§î **Pensamento:** Enviando o resultado para o S√°bio fazer a interpreta√ß√£o final.")
        final_answer = get_sage_interpretation(llm, tool_result, clarified_query)
        artifact_path = tool_result if intended_tool == "Visualizer" else None
        return {"text_answer": final_answer, "artifact_path": artifact_path, "thoughts": log_entries}

    tools = [
        {"name": "DataGuardian", "description": "√ötil para responder a perguntas que exigem an√°lise de dados brutos, c√°lculos ou estat√≠sticas."},
        {"name": "Visualizer", "description": "√ötil para criar uma visualiza√ß√£o de dados, um gr√°fico ou um plot."},
        {"name": "GeneralConversation", "description": "Use para sauda√ß√µes ou perguntas gerais que n√£o s√£o sobre os dados."}
    ]

    try:
        df_profile = get_data_profile(df)
        tools_json = json.dumps(tools, indent=2, ensure_ascii=False)
        tool_selection_prompt = f'''Sua tarefa √© selecionar a ferramenta mais apropriada para responder √† pergunta do usu√°rio. Voc√™ DEVE seguir estas regras:
1. Analise o Perfil do DataFrame abaixo. Se a pergunta do usu√°rio mencionar qualquer nome de coluna ou se referir a caracter√≠sticas dos dados (como 'colunas', 'linhas', 'tipos de dados', 'distribui√ß√£o', etc.), voc√™ DEVE escolher 'DataGuardian' ou 'Visualizer'.
2. Apenas se a pergunta for uma sauda√ß√£o ou claramente n√£o relacionada aos dados, escolha 'GeneralConversation'.

### Perfil do DataFrame
{df_profile}

Ferramentas dispon√≠veis:
{tools_json}

Pergunta do usu√°rio: "{user_query}"

Ferramenta selecionada (JSON):
'''
        
        log("ü§î **Pensamento:** Analisando a inten√ß√£o do usu√°rio para selecionar a ferramenta correta.")

        # L√≥gica de Curto-Circuito para for√ßar a sele√ß√£o da ferramenta correta
        query_lower = user_query.lower()
        viz_keywords = ['gr√°fico', 'plot', 'visualize', 'visualiza√ß√£o', 'histograma', 'barras', 'pizza', 'scatterplot', 'boxplot']
        data_keywords = ['coluna', 'colunas', 'dado', 'dados', 'tipo', 'tipos', 'distribui√ß√£o', 'correla√ß√£o', 'm√©dia', 'mediana']

        tool_name = ""
        if any(keyword in query_lower for keyword in viz_keywords):
            tool_name = "Visualizer"
            log("üí° **Curto-circuito:** Pergunta de visualiza√ß√£o detectada. For√ßando o uso do Visualizer.")
        elif any(keyword in query_lower for keyword in data_keywords) or any(col.lower() in query_lower for col in df.columns):
            tool_name = "DataGuardian"
            log("üí° **Curto-circuito:** Pergunta de an√°lise de dados detectada. For√ßando o uso do DataGuardian.")
        
        # Se nenhuma palavra-chave foi detectada, use o LLM para classifica√ß√£o
        if not tool_name:
            log("ü§î **Pensamento:** Nenhuma palavra-chave detectada. Usando LLM para classifica√ß√£o de inten√ß√£o.")
            response = llm.invoke(tool_selection_prompt)
            tool_choice_str = response.content.strip()
            try:
                if tool_choice_str.startswith("```json"):
                    tool_choice_str = tool_choice_str[7:-3].strip()
                tool_choice = json.loads(tool_choice_str)
                if isinstance(tool_choice, dict):
                    tool_name = tool_choice.get('tool_name')
            except (json.JSONDecodeError, AttributeError):
                if "DataGuardian" in tool_choice_str:
                    tool_name = "DataGuardian"
                elif "Visualizer" in tool_choice_str:
                    tool_name = "Visualizer"
                elif "GeneralConversation" in tool_choice_str:
                    tool_name = "GeneralConversation"
            if not tool_name:
                tool_name = "GeneralConversation"
        log(f"ü§î **Pensamento:** A inten√ß√£o parece ser '{tool_name}'.")

        if tool_name == "GeneralConversation":
            log("üé¨ **A√ß√£o:** A pergunta √© uma conversa geral. Acionando o modo de conversa√ß√£o.")
            response_text = handle_general_conversation(llm, user_query)
            return {"text_answer": response_text, "artifact_path": None, "thoughts": log_entries}

        guidance_prompt = f"""
        Voc√™ √© um consultor de ci√™ncia de dados s√™nior. A pergunta do usu√°rio √©: '{user_query}'. A ferramenta selecionada √© '{tool_name}'.
        Sua tarefa √© dupla:
        1. Avalie se a pergunta √© amb√≠gua (ex: "mostre a distribui√ß√£o" sem especificar o tipo de gr√°fico).
        2. Mesmo que clara, avalie se existe uma abordagem melhor (ex: sugerir um gr√°fico de barras em vez de um de pizza para duas categorias).

        Se a pergunta for clara e ideal, responda com JSON: {{"action": "proceed"}}.
        Se for amb√≠gua ou puder ser melhorada, formule uma sugest√£o/pergunta e responda com JSON: {{"action": "clarify", "response": "Sua sugest√£o ou pergunta aqui."}}.

        Exemplo (Sugest√£o): Pergunta: "gr√°fico de pizza da coluna 'Class'". Resposta: {{"action": "clarify", "response": "Um gr√°fico de pizza pode ser usado, mas para comparar apenas duas categorias como na coluna 'Class', um gr√°fico de barras costuma ser mais claro e eficaz. Voc√™ gostaria que eu gerasse um gr√°fico de barras em vez disso?"}}
        Exemplo (Clara): Pergunta: "crie um histograma da coluna 'Amount'". Resposta: {{"action": "proceed"}}

        Sua avalia√ß√£o para a pergunta "{user_query}":
        """
        log("ü§î **Pensamento:** Verificando se a pergunta √© clara ou se posso sugerir uma abordagem melhor.")
        guidance_response = llm.invoke(guidance_prompt)
        guidance_check = json.loads(guidance_response.content.strip().replace("```json", "").replace("```", ""))

        if guidance_check.get("action") == "clarify":
            log(f"üé¨ **A√ß√£o:** A pergunta √© amb√≠gua/pode ser melhorada. Pedindo esclarecimento ao usu√°rio.")
            st.session_state.pending_clarification = {"original_query": user_query, "intended_tool": tool_name}
            return {"text_answer": guidance_check["response"], "artifact_path": None, "thoughts": log_entries}
        
        log(f"ü§î **Pensamento:** A pergunta √© clara. Acionando a ferramenta '{tool_name}'.")
        log(f"üé¨ **A√ß√£o:** Acionando a ferramenta `{tool_name}`.")
        tool_response = {}
        if tool_name == "DataGuardian":
            tool_response = run_guardian_query(llm, df, user_query, record_thoughts)
        elif tool_name == "Visualizer":
            tool_response = create_static_plot(llm, df, user_query, record_thoughts)
        
        tool_result = tool_response.get("result", "")
        specialist_thoughts = tool_response.get("thoughts", "")

        if record_thoughts and specialist_thoughts:
            log(f"--- In√≠cio do Log Detalhado de {tool_name} ---")
            log(f"```\n{specialist_thoughts}\n```")
            log(f"--- Fim do Log Detalhado de {tool_name} ---")

        log(f"üîç **Observa√ß√£o:** A ferramenta '{tool_name}' retornou um resultado.")

        log("ü§î **Pensamento:** Enviando o resultado para o S√°bio fazer a interpreta√ß√£o final.")
        log("üé¨ **A√ß√£o:** Acionando a ferramenta `DataSage`.")
        final_answer = get_sage_interpretation(llm, tool_result, user_query)
        artifact_path = tool_result if tool_name == "Visualizer" else None
        return {"text_answer": final_answer, "artifact_path": artifact_path, "thoughts": log_entries}

    except Exception as e:
        return {"text_answer": f"O Conselho Jedi encontrou uma perturba√ß√£o na For√ßa. Um erro cr√≠tico ocorreu: {str(e)}", "artifact_path": None, "thoughts": log_entries}